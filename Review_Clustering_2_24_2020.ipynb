{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import jaydebeapi\n",
    "import ConfigParser\n",
    "import sys\n",
    "import urllib\n",
    "import requests\n",
    "pd.options.display.max_columns = None\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import line_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from palettable.colorbrewer.sequential import Reds_9\n",
    "import random\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "##https://minimaxir.com/2016/05/wordclouds/\n",
    "##https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/exue001c/evagit/amazon_healthcare_review/data/review_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_len</th>\n",
       "      <th>title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>one_gram_lema_text</th>\n",
       "      <th>n_gram_lema_text</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>one_gram_lema_text.1</th>\n",
       "      <th>n_gram_lema_text.1</th>\n",
       "      <th>orig_text.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>B07H7Q1DFP</td>\n",
       "      <td>4</td>\n",
       "      <td>Got these in hopes that my hands would not hur...</td>\n",
       "      <td>1553126400000</td>\n",
       "      <td>418</td>\n",
       "      <td>Very soft, saves the hands and under arms</td>\n",
       "      <td>41</td>\n",
       "      <td>got hope hand would hurt use comfortable hand ...</td>\n",
       "      <td>got_hope hope_hand hand_would would_not not_hu...</td>\n",
       "      <td>Got these in hopes that my hands would not hur...</td>\n",
       "      <td>soft save hand arm</td>\n",
       "      <td>soft_save save_hand hand_arm</td>\n",
       "      <td>Very soft, saves the hands and under arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>B07HM7P88N</td>\n",
       "      <td>5</td>\n",
       "      <td>The product came very fast and it is very cush...</td>\n",
       "      <td>1557964800000</td>\n",
       "      <td>94</td>\n",
       "      <td>It does work!</td>\n",
       "      <td>13</td>\n",
       "      <td>product come fast cushy palm hand armpit</td>\n",
       "      <td>product_come come_fast fast_cushy cushy_palm p...</td>\n",
       "      <td>The product came very fast and it is very cush...</td>\n",
       "      <td>work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It does work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B07HM7P88N</td>\n",
       "      <td>5</td>\n",
       "      <td>My daughter had to look for support for her cr...</td>\n",
       "      <td>1553385600000</td>\n",
       "      <td>143</td>\n",
       "      <td>My daughter loves it. Best support for your cr...</td>\n",
       "      <td>53</td>\n",
       "      <td>baby save armpit heel hand recover knee surger...</td>\n",
       "      <td>baby_save save_armpit armpit_heel heel_hand ha...</td>\n",
       "      <td>These babies have saved my armpits and the hee...</td>\n",
       "      <td>comfort longer term use</td>\n",
       "      <td>comfort_longer longer_term term_use</td>\n",
       "      <td>Comfort for longer term crutch use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>B01IBUD24G</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm a student in college, and this was super h...</td>\n",
       "      <td>1523577600000</td>\n",
       "      <td>104</td>\n",
       "      <td>and this was super helpful to carry around my ...</td>\n",
       "      <td>71</td>\n",
       "      <td>worked great</td>\n",
       "      <td>worked_great</td>\n",
       "      <td>Worked out great.</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>B01IBUD24G</td>\n",
       "      <td>4</td>\n",
       "      <td>The crutch bag gets five stars for being super...</td>\n",
       "      <td>1500076800000</td>\n",
       "      <td>595</td>\n",
       "      <td>Great idea and super handy, but recommended wi...</td>\n",
       "      <td>62</td>\n",
       "      <td>purchase daughter surgery leg helpful add comf...</td>\n",
       "      <td>purchase_daughter daughter_surgery surgery_leg...</td>\n",
       "      <td>I purchased these for my daughter when she had...</td>\n",
       "      <td>nice product</td>\n",
       "      <td>nice_product</td>\n",
       "      <td>Very Nice Product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  helpful_votes  product_id  rating  \\\n",
       "0           0              0  B07H7Q1DFP       4   \n",
       "1           1              0  B07HM7P88N       5   \n",
       "2           2              0  B07HM7P88N       5   \n",
       "3           3              0  B01IBUD24G       5   \n",
       "4           4              3  B01IBUD24G       4   \n",
       "\n",
       "                                              review    review_date  \\\n",
       "0  Got these in hopes that my hands would not hur...  1553126400000   \n",
       "1  The product came very fast and it is very cush...  1557964800000   \n",
       "2  My daughter had to look for support for her cr...  1553385600000   \n",
       "3  I'm a student in college, and this was super h...  1523577600000   \n",
       "4  The crutch bag gets five stars for being super...  1500076800000   \n",
       "\n",
       "   review_len                                              title  title_len  \\\n",
       "0         418          Very soft, saves the hands and under arms         41   \n",
       "1          94                                      It does work!         13   \n",
       "2         143  My daughter loves it. Best support for your cr...         53   \n",
       "3         104  and this was super helpful to carry around my ...         71   \n",
       "4         595  Great idea and super handy, but recommended wi...         62   \n",
       "\n",
       "                                  one_gram_lema_text  \\\n",
       "0  got hope hand would hurt use comfortable hand ...   \n",
       "1           product come fast cushy palm hand armpit   \n",
       "2  baby save armpit heel hand recover knee surger...   \n",
       "3                                       worked great   \n",
       "4  purchase daughter surgery leg helpful add comf...   \n",
       "\n",
       "                                    n_gram_lema_text  \\\n",
       "0  got_hope hope_hand hand_would would_not not_hu...   \n",
       "1  product_come come_fast fast_cushy cushy_palm p...   \n",
       "2  baby_save save_armpit armpit_heel heel_hand ha...   \n",
       "3                                       worked_great   \n",
       "4  purchase_daughter daughter_surgery surgery_leg...   \n",
       "\n",
       "                                           orig_text     one_gram_lema_text.1  \\\n",
       "0  Got these in hopes that my hands would not hur...       soft save hand arm   \n",
       "1  The product came very fast and it is very cush...                     work   \n",
       "2  These babies have saved my armpits and the hee...  comfort longer term use   \n",
       "3                                  Worked out great.                     good   \n",
       "4  I purchased these for my daughter when she had...             nice product   \n",
       "\n",
       "                    n_gram_lema_text.1  \\\n",
       "0         soft_save save_hand hand_arm   \n",
       "1                                  NaN   \n",
       "2  comfort_longer longer_term term_use   \n",
       "3                                  NaN   \n",
       "4                         nice_product   \n",
       "\n",
       "                                 orig_text.1  \n",
       "0  Very soft, saves the hands and under arms  \n",
       "1                              It does work!  \n",
       "2         Comfort for longer term crutch use  \n",
       "3                                  All Good.  \n",
       "4                          Very Nice Product  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = df.one_gram_lema_text.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2478, 1000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,max_features=no_features)\n",
    "X_tfidf = tf_idf_vectorizer.fit_transform(review_list)\n",
    "##tf_idf_norm = normalize(X_tfidf)\n",
    "#tf_idf_array = tf_idf_norm.toarray()\n",
    "tfidf_feature = tf_idf_vectorizer.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns = tfidf_feature)\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse document frequency is defined as:\n",
    "\n",
    "ùëñùëëùëì(ùë°,ùê∑)=ùëôùëúùëîùëÅ|ùëë‚ààùê∑:ùë°‚ààùëë|\n",
    "\n",
    "Where ùëÅ is the total number of documents in the corpus, and |ùëë‚ààùê∑:ùë°‚ààùëë| is the number of documents where ùë° appears.\n",
    "\n",
    "TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data.\n",
    "\n",
    "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "To overcome this , we use TfidfVectorizer .\n",
    "\n",
    "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2478, 1000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(max_df=0.95, min_df=2,max_features=no_features)\n",
    "#vec = CountVectorizer(binary=True)\n",
    "X_tf = vec.fit_transform(review_list)\n",
    "tf_feature = vec.get_feature_names()\n",
    "tf_df = pd.DataFrame(X_tf.toarray(), columns = tf_feature)\n",
    "tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation , NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NMF -  relies on linear algebra\n",
    " Non-negative Matrix Factorization (NMF)\n",
    " \n",
    " NMF is able to use tf-idf\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - is based on probabilistic graphical modeling \n",
    "LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/exue001c/miniconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run NMF\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(X_tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(X_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "recommend highly color like would soft new expect anyone attach\n",
      "Topic #1:\n",
      "get use hand go would time cushion week armpit one\n",
      "Topic #2:\n",
      "fit great work carry phone well good use bag product\n",
      "Topic #3:\n",
      "comfortable make use easy love daughter hand much arm help\n",
      " \n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "use make easy hand get much love arm well help\n",
      "Topic #1:\n",
      "comfortable make much made definitely super easy use daughter soft\n",
      "Topic #2:\n",
      "great product work worked price quality value item comfort seller\n",
      "Topic #3:\n",
      "fit perfect perfectly well work old good replacement quality like\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "print_top_words(lda, tf_feature, n_top_words)\n",
    "print(\"Topics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_feature, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "https://stackabuse.com/implementing-lda-in-python-with-scikit-learn/\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#5.-Build-the-Topic-Model\n",
    "https://medium.com/@sherryqixuan/topic-modeling-and-pyldavis-visualization-86a543e21f58\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.sklearn.prepare(lda, X_lemma, vec)\n",
    "pyLDAvis.save_html(p,'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_html() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b20813041817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_html() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "pyLDAvis.save_html(p,'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1 - Review ont the product quality eg., \n",
    "Topic 2\n",
    "Topic 3\n",
    "Topic 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
